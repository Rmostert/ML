{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we'll explore the batch learning, stochastic gradient descent (SGD) and mini batch algorithms for a linear regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T19:26:44.409335Z",
     "start_time": "2018-05-09T19:26:44.404165Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import rand, randn, randint,seed, permutation\n",
    "from numpy.linalg import inv\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from __future__ import division, print_function\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start of by generating some pseudo data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T19:21:15.883234Z",
     "start_time": "2018-05-09T19:21:15.877136Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed(555)\n",
    "X = 2 * rand(100,1)\n",
    "y = 4 + 3 * X + rand(100,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create the design matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T19:21:15.892320Z",
     "start_time": "2018-05-09T19:21:15.887869Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_b = np.c_[np.ones((100,1)),X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The closed solution can easily be calculated by using the following formula. Alas, in the case of thousand of variables, it may not be optimal. That is where the SGD algoritm comes in handy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T19:21:15.900844Z",
     "start_time": "2018-05-09T19:21:15.896059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to complete: 0.140156984329\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "theta_best = inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "print('Time to complete: {}'.format(time()-start))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T19:21:15.922001Z",
     "start_time": "2018-05-09T19:21:15.908008Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.57407633],\n",
       "       [ 2.96585269]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intercept is estimated to be 4.57 and the coefficient 2.97. This is close to the actual parameter values. Nexxt we'll make use of the batch gradient descent algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with a 1000 iterations and a learning rate equal to 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T19:21:15.931901Z",
     "start_time": "2018-05-09T19:21:15.925584Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed(355)\n",
    "eta = 0.1\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "theta = randn(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T19:21:15.957923Z",
     "start_time": "2018-05-09T19:21:15.935173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to complete: 0.010883808136\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "print('Time to complete: {}'.format(time()-start))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.57407633],\n",
       "       [ 2.96585269]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exaclty the same vlaues we got from the closed solution. We can also create a method to make changing the number of iterations and learning rate easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch_learning():\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return X.dot(self.theta)\n",
    "    \n",
    "    def fit(self,X,Y,eta=0.1,n_iterations=1000):\n",
    "        \n",
    "        X_b = np.c_[np.ones((X.shape[0],1)),X]\n",
    "        self.theta = randn(X_b.shape[1],1)\n",
    "        m = X_b.shape[0]\n",
    "        \n",
    "        for iteration in range(n_iterations):\n",
    "            y_hat = self.predict(X_b)\n",
    "            gradients = 2/m * X_b.T.dot(y_hat - y)\n",
    "            self.theta = self.theta - eta * gradients\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_learn_mod = Batch_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_learn_mod.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.57407633],\n",
       "       [ 2.96585269]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_learn_mod.theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the stochastic gradient descent algorithm, we'll start with a relatively high learning rate and then decrease it per epoch. The number of epochs will be set equal to 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD():\n",
    "    \n",
    "    def learning_schedule(self,iteration):\n",
    "        return self.learning_rate * (1 / (1+ self.decay * iteration))\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return X.dot(self.theta)\n",
    "    \n",
    "    def fit(self,X,Y,eta0=0.1,n_iterations=50):\n",
    "        \n",
    "        self.iterations = n_iterations\n",
    "        self.decay = eta0 / n_iterations\n",
    "        self.learning_rate = eta0\n",
    "        \n",
    "        X_b = np.c_[np.ones((X.shape[0],1)),X]\n",
    "        self.theta = randn(X_b.shape[1],1)\n",
    "        m = X_b.shape[0]\n",
    "        \n",
    "        for iteration in range(n_iterations):\n",
    "            \n",
    "            self.learning_rate = self.learning_schedule(iteration)\n",
    "            \n",
    "            for i in range(m):\n",
    "                random_index = randint(m)\n",
    "                xi = X_b[random_index:random_index+1]\n",
    "                yi = y[random_index:random_index+1]\n",
    "                y_hat = self.predict(xi)   \n",
    "                gradients = 2 * xi.T.dot(y_hat - yi)\n",
    "                self.theta = self.theta - self.learning_rate * gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T19:21:16.129388Z",
     "start_time": "2018-05-09T19:21:16.121947Z"
    }
   },
   "outputs": [],
   "source": [
    "SGD_mod = SGD()\n",
    "SGD_mod.fit(X,y,eta0=0.1,n_iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.57515336],\n",
       "       [ 2.97866398]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SGD_mod.theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very close to the closed solution. We can of course also just use sklearn's SGDRegressor method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T19:21:16.144564Z",
     "start_time": "2018-05-09T19:21:16.132918Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.1,\n",
       "       fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
       "       loss='squared_loss', max_iter=50, n_iter=None, penalty=None,\n",
       "       power_t=0.25, random_state=None, shuffle=True, tol=None, verbose=0,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg = SGDRegressor(max_iter = 50, penalty=None, eta0=0.1)\n",
    "sgd_reg.fit(X,y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T19:21:16.155693Z",
     "start_time": "2018-05-09T19:21:16.149015Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 4.56618965]), array([ 2.95393607]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we'll have a look at the mini-batch algorithm. We'll make use of batch sizes of 20, start with a relatively high learning rate of 0.2 and have 50 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T19:29:32.374312Z",
     "start_time": "2018-05-09T19:29:32.370468Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T19:29:32.883883Z",
     "start_time": "2018-05-09T19:29:32.832367Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed(5445)\n",
    "theta = randn(2,1)\n",
    "t = 0\n",
    "t0, t1 = 200, 1000 #learning schedule hyperparameter\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    shuffled_indices = permutation(m)\n",
    "    X_b_shuffled = X_b[shuffled_indices]\n",
    "    y_shuffled = y[shuffled_indices]\n",
    "    \n",
    "    for i in range(0,m,batch_size):\n",
    "        t +=1\n",
    "        xi = X_b[i:i + batch_size]\n",
    "        yi = y[i:i + batch_size]\n",
    "        gradients = 2/batch_size * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(t)\n",
    "        theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T19:28:50.864291Z",
     "start_time": "2018-05-09T19:28:50.858525Z"
    }
   },
   "outputs": [],
   "source": [
    "class mini_batch():\n",
    "    \n",
    "    def learning_schedule(self,iteration):\n",
    "        return self.learning_rate * (1 / (1+ self.decay * iteration))\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return X.dot(self.theta)\n",
    "    \n",
    "    def fit(self,X,Y,eta0=0.1,n_iterations=50,batch_size=20):\n",
    "        \n",
    "        self.iterations = n_iterations\n",
    "        self.decay = eta0 / n_iterations\n",
    "        self.learning_rate = eta0\n",
    "        \n",
    "        X_b = np.c_[np.ones((X.shape[0],1)),X]\n",
    "        self.theta = randn(X_b.shape[1],1)\n",
    "        m = X_b.shape[0]\n",
    "        \n",
    "        for iteration in range(n_iterations):\n",
    "            \n",
    "            shuffled_indices = permutation(m)\n",
    "            X_b_shuffled = X_b[shuffled_indices]\n",
    "            y_shuffled = y[shuffled_indices]\n",
    "            \n",
    "            self.learning_rate = self.learning_schedule(iteration)\n",
    "            \n",
    "            for i in range(0,m,batch_size):\n",
    "    \n",
    "                xi = X_b[i:i + batch_size]\n",
    "                yi = y[i:i + batch_size]\n",
    "                y_hat = self.predict(xi)   \n",
    "                gradients = 2/batch_size * xi.T.dot(y_hat - yi)\n",
    "                self.theta = self.theta - self.learning_rate * gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD_mod = mini_batch()\n",
    "SGD_mod.fit(X,y,eta0=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.57590897],\n",
       "       [ 2.96586903]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SGD_mod.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
